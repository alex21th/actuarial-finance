knitr::opts_chunk$set(echo = TRUE)
plot(serie, main="Paro registrado en España (miles de personas)")
abline(v=1990:2019, col=4, lty=3)
plot(decompose(serie))
m <- apply(matrix(serie, nrow = 12), 2, mean)
v <- apply(matrix(serie, nrow = 12), 2, var)
plot(m,v)
groups <- floor(time(serie))
boxplot(serie~groups)
library("MASS")
bc <- boxcox(serie~1)
ind <- which.max(bc$y)
lambd <- bc$x[ind]
lnserie <- log(serie)
groups <- (floor(time(serie)))
m <- apply(matrix(lnserie, nrow = 12), 2, mean)
v <- apply(matrix(lnserie, nrow = 12), 2, var)
plot(m,v)
monthplot(lnserie)
d12lnserie <- diff(lnserie,12)
var(d12lnserie)
d1d12lnserie <- diff(d12lnserie)
var(d1d12lnserie)
d1d1d12lnserie <- diff(d1d12lnserie)
var(d1d1d12lnserie)
plot(d1d12lnserie)
m1 <- arima(lnserie, order = c(1,1,1),
seasonal = list(order = c(0,1,1), period = 12))
m2 <- arima(lnserie, order = c(8,1,0),
seasonal = list(order = c(0,1,1), period = 12))
m1 <- arima(lnserie, order = c(1,1,1),
seasonal = list(order = c(0,1,1), period = 12))
m2 <- arima(lnserie, order = c(8,1,0),
seasonal = list(order = c(0,1,1), period = 12))
scatter.smooth(resid(m1),lpars=list(col=2))
scatter.smooth(sqrt(abs(resid(m1))),lpars=list(col=2))
qqnorm(resid(m1))
qqline(resid(m1),col=2,lwd=2)
hist(resid(m1), breaks = 20)
hist(sqrt(resid(m1)), breaks = 10)
hist(resid(m1), breaks = 20)
curve(dnorm(x,mean = mean(resid(m1)), sd=sd(resid(m1))),col=2,add=T)
shapiro.test(resid(m1))
par(mfrow = c(1,2))
acf(resid(m1), ylim = c(-1,1), lwd = 2,
lag.max = 72, col = c(2, rep(1,11)))
pacf(resid(m1), ylim = c(-1,1), lwd = 2,
lag.max = 72, col = c(rep(1,11),2))
par(mfrow = c(1,1))
Box.test(resid(m1))
par(mfrow = c(1,2))
acf(resid(m1), ylim = c(-1,1), lwd = 2,
lag.max = 72, col = c(2, rep(1,11)))
pacf(resid(m1), ylim = c(-1,1), lwd = 2,
lag.max = 72, col = c(rep(1,11),2))
par(mfrow = c(1,1))
tsdiag(m1, gof.lag = 72)
tsdiag(m1, gof.lag = 72)
knitr::opts_chunk$set(echo = TRUE)
tsdiag(m1, gof.lag = 72)
m1$model$phi
Mod(polyroot(-m1$model$phi))
all(Mod(polyroot(m1$model$theta)) > 1) # No invertible
all(Mod(polyroot(-m2$model$phi)) > 1) # No causal
all(Mod(polyroot(m2$model$theta)) > 1) # No invertible
Mod(polyroot(c(1, m1$model$theta)))
all(Mod(polyroot(c(1, m1$model$theta))) > 1)
all(Mod(polyroot(c(1, m1$model$phi))) > 1) # Hi ha invertibilitat
all(Mod(polyroot(c(1, m1$model$phi))) > 1) # Hi ha invertibilitat
all(Mod(polyroot(m1$model$theta)) > 1) # No invertible
all(Mod(polyroot(c(1, -m1$model$phi))) > 1) # És NO causal
all(Mod(polyroot(c(1, -m1$model$phi))) > 1) # És NO causal
all(Mod(polyroot(c(1, -m1$model$phi))) > 1) # És NO causal
all(Mod(polyroot(c(1, m2$model$theta))) > 1) # Hi ha invertibilitat
all(Mod(polyroot(c(1, -m2$model$phi))) > 1) # És NO causal
Mod(polyroot(c(1, -m2$model$phi)))
od(polyroot(m2$model$theta))
Mod(polyroot(m2$model$theta))
Mod(polyroot(-m2$model$phi))
m1$model$phi
all(Mod(polyroot(c(1, m1$model$theta))) > 1) # Hi ha invertibilitat
all(Mod(polyroot(c(1, -m1$model$phi))) > 1) # És NO causal
all(Mod(polyroot(c(1, m2$model$theta))) > 1) # Hi ha invertibilitat
all(Mod(polyroot(c(1, -m2$model$phi))) > 1) # És NO causal
all(Mod(polyroot(c(1, m1$model$theta))) > 1) # Hi ha invertibilitat
all(Mod(polyroot(c(1, -m1$model$phi))) > 1) # És NO causal
all(Mod(polyroot(c(1, m2$model$theta))) > 1) # Hi ha invertibilitat
all(Mod(polyroot(c(1, -m2$model$phi))) > 1) # És NO causal
AIC(m1)
BIC(m1)
AIC(m1)
BIC(m1)
AIC(m2)
BIC(m2)
m1
m2
library(tree)
install.packages("tree")
set.seed (6046)
library(kernlab)
data(spam)
spam[,55:57] <- as.matrix(log10(spam[,55:57]+1))
spam2 <- spam[spam$george==0,]
spam2 <- spam2[spam2$num650==0,]
spam2 <- spam2[spam2$hp==0,]
spam2 <- spam2[spam2$hpl==0,]
george.vars <- 25:28
spam2 <- spam2[,-george.vars]
set.seed (6046)
library(kernlab)
install.packages("kernlab")
set.seed (6046)
library(kernlab)
data(spam)
spam[,55:57] <- as.matrix(log10(spam[,55:57]+1))
spam2 <- spam[spam$george==0,]
spam2 <- spam2[spam2$num650==0,]
spam2 <- spam2[spam2$hp==0,]
spam2 <- spam2[spam2$hpl==0,]
george.vars <- 25:28
spam2 <- spam2[,-george.vars]
moneys.vars <- c(16,17,20,24)
spam3 <- data.frame( spam2[,-moneys.vars], spam2[,16]+spam2[,17]+spam2[,20]+spam2[,24])
colnames(spam3)[51] <- "about.money"
dim(spam3)
summary(spam3)
set.seed (4321)
N <- nrow(spam3)
learn <- sample(1:N, round(0.67*N))
nlearn <- length(learn)
library(tree)
model.tree <- tree (type ~ ., data=spam3[learn,])
summary(model.tree)
model.tree
plot (model.tree)
text (model.tree,pretty=0)
pred.tree <- predict (model.tree, spam3[-learn,], type="class")
(ct <- table(Truth=spam3[-learn,]$type, Pred=pred.tree))
harm <- function (a,b) { 2/(1/a+1/b) }
# percent by class
prop.table(ct, 1)
# total percent correct
sum(diag(ct))/sum(ct)
round(100*(1-sum(diag(ct))/sum(ct)),2)
(F1 <- harm (prop.table(ct,1)[1,1], prop.table(ct,1)[2,2]))
## Now a random Forest
library(randomForest)
model.rf1 <- randomForest (type ~ ., data=spam3[learn,], ntree=100, proximity=FALSE)
install.packages("randomForest")
model.rf1
pred.rf1 <- predict (model.rf1, spam3[-learn,], type="class")
## Now a random Forest
library(randomForest)
model.rf1 <- randomForest (type ~ ., data=spam3[learn,], ntree=100, proximity=FALSE)
model.rf1
View(aics)
View(aics)
library(MASS)
library(nnet)
Admis <- read.csv("/Users/Alex/MacBook Pro/Documentos UPC/2n Curs - 2018:2019/APRENENTATGE AUTOMÀTIC 1/Pràctiques/L08/Admissions.csv")
## view the first few rows of the data
head(Admis)
Admis$admit <- factor(Admis$admit, labels=c("No","Yes"))
summary(Admis)
N <- nrow(Admis)
set.seed(43)
learn <- sample(1:N, round(2*N/3))
nlearn <- length(learn)
ntest <- N - nlearn
Admis$gpa <- scale(Admis$gpa)
Admis$gre <- scale(Admis$gre)
Admis$rank <- scale(Admis$rank)
model.nnet0 <- multinom (admit ~., data = Admis, subset=learn, maxit=200)
# Now let's compute the training error and test errors
errors <- function (model)
{
options(digits=4)
p1 <- as.factor(predict (model, type="class"))
t1 <- table(p1,Admis$admit[learn])
cat ("Train = ", 100*(1-sum(diag(t1))/nlearn),"%\n")
p2 <- as.factor(predict (model, newdata=Admis[-learn,], type="class"))
t2 <- table(p2,Admis$admit[-learn])
cat ("Test = ", 100*(1-sum(diag(t2))/ntest),"%\n")
}
errors (model.nnet0)
model.nnet <- nnet(admit ~., data = Admis, subset=learn, size=2, maxit=200, decay=0)
## Take your time to understand the output
model.nnet
# This is the fitting criterion (aka error function)
model.nnet$value
#  fitted values for the training data
model.nnet$fitted.values
# and the residuals
model.nnet$residuals
model.nnet$wts
summary(model.nnet)
model.nnet <- nnet(admit ~., data = Admis, subset=learn, size=2, maxit=200, decay=0.01)
## notice the big difference
summary(model.nnet)
# Now let's compute the errors
errors (model.nnet)
model.nnet <- nnet(admit ~., data = Admis, subset=learn, size=20, maxit=200)
errors (model.nnet)
## WARNING: if the package is not installed in your computer, installation takes quite a while
library(caret)
(sizes <- 2*seq(1,10,by=1))
## specify 10x10 CV
trc <- trainControl (method="repeatedcv", number=10, repeats=10)
model.10x10CV <- train (admit ~., data = Admis, subset=learn, method='nnet', maxit = 500, trace = FALSE,
tuneGrid = expand.grid(.size=sizes,.decay=0), trControl=trc)
## We can inspect the full results
model.10x10CV$results
## and the best model found
model.10x10CV$bestTune
(decays <- 10^seq(-3,0,by=0.5))
## WARNING: this takes a few minutes
model.10x10CV <- train (admit ~., data = Admis, subset=learn, method='nnet', maxit = 500, trace = FALSE,
tuneGrid = expand.grid(.size=20,.decay=decays), trControl=trc)
## We can inspect the full results
model.10x10CV$results
(decays <- 10^seq(-3,0,by=0.5))
## and the best model found
model.10x10CV$bestTune
predict (model.10x10CV, newdata=Admis[-learn,], type="raw")
as.factor(predict (model.10x10CV, newdata=Admis[-learn,], type="raw"))
## So what remains is to predict the test set with our final model
p2 <- as.factor(predict (model.10x10CV, newdata=Admis[-learn,], type="raw"))
t2 <- table(pred=p2,truth=Admis$admit[-learn])
t2
(error_rate.test <- 100*(1-sum(diag(t2))/ntest))
## ... upon looking at the confusion matrix for the predictions ...
t2
decays <- seq(from = 0.1, to = 0.5, by = 0.1)
decays
(decays <- 10^seq(-3,0,by=0.5))
# ----------------------------------------------------
# Oversampling                      (dd_balanced_over)
# ----------------------------------------------------
(decays <- 10^seq(-3,0,by=0.5))
trc <- trainControl (method="repeatedcv", number=10, repeats=10)
load("/Users/Alex/MacBook Pro/Documentos UPC/2n Curs - 2018:2019/APRENENTATGE AUTOMÀTIC 1/Projecte/entregable/lda_works.RData")
# ----------------------------------------------------
# Oversampling                      (dd_balanced_over)
# ----------------------------------------------------
(decays <- 10^seq(-3,0,by=0.5))
trc <- trainControl (method="repeatedcv", number=10, repeats=10)
# ----------------------------------------------------
# Oversampling                      (dd_balanced_over)
# ----------------------------------------------------
model.nnet <- nnet(y ~., data = dd_balanced_over, size=2, maxit=200, decay=0)
summary(model.nnet)
View(model.nnet)
View(model.nnet)
round(51/2)
round(47/2)
?predict
# ----------------------------------------------------
# Oversampling                      (dd_balanced_over)
# ----------------------------------------------------
model.nnet <- nnet(y ~., data = dd_balanced_over, size=2, maxit=200, decay=0)
summary(model.nnet)
useless.nnet <- nnet(y ~. data = training, size = 2, maxit = 200, decay = 0)
useless.nnet <- nnet(y ~., data = training, size = 2, maxit = 200, decay = 0)
View(useless.nnet)
View(useless.nnet)
useless.nnet$n
M <- useless.nnet$n[1]
M
(decays <- 10^seq(-3,0,by=0.5))
(decays <- 10^seq(-3,0,by=0.5))
trc <- trainControl(method="repeatedcv", number=10, repeats=10)
H <- round(M/2)
H
(decays <- 10^seq(-3, 0, by = 0.5))
numFolds <- trainControl(method = "repeatedcv", number = 10, repeats = 10)
library(MASS)
library(nnet)  # For the NN itself.
library(caret) # For the CV of model parameters.
useless.nnet <- nnet(y ~., data = training, size = 2, maxit = 200, decay = 0)
useless.nnet$n
M <- useless.nnet$n[1]
H <- round(M/2)
(decays <- 10^seq(-3, 0, by = 0.5))
numFolds <- trainControl(method = "repeatedcv", number = 10, repeats = 10)
## WARNING: this takes a few minutes
over.nnet <- train (y ~., data = dd_balanced_over, method = 'nnet',
maxit = 500, trace = FALSE, trControl = numFolds,
tuneGrid = expand.grid(.size=H,.decay=decays))
library(shiny); runApp('Desktop/dashbuyer.R')
install.packages("shinydashboard")
runApp('Desktop/dashbuyer.R')
ProjectPath()
ProjectPath <- function(path = rstudioapi::getActiveProject()){
path <- rstudioapi::selectDirectory(caption="Select the raw file directory",path=path)
return(glue(path,"/"))
}
ProjectPath()
library(glue)
ProjectPath <- function(path = rstudioapi::getActiveProject()){
path <- rstudioapi::selectDirectory(caption="Select the raw file directory",path=path)
return(glue(path,"/"))
}
ProjectPath()
ProjectPath <- function(path = rstudioapi::getActiveProject()){
path <- rstudioapi::selectDirectory(caption="Select the raw file directory",path=path)
return(glue(path,"/"))
}
ProjectPath()
# Dependences ###
library(shinydashboard)
library(shiny)
library(glue)
LoadObject <- function(path = ProjectFile) {
object <- read.csv(path)
return(object)
}
matches <- LoadObject()
ProjectFile <- function(path = rstudioapi::getActiveProject()){
path <- rstudioapi::selectFile(caption="Select the raw file directory",path=path)
return(path)
}
LoadObject <- function(path = ProjectFile) {
object <- read.csv(path)
return(object)
}
matches <- LoadObject()
LoadObject <- function(path = ProjectFile()) {
object <- read.csv(path)
return(object)
}
matches <- LoadObject()
View(matches)
matches_nomargin <- LoadObject()
View(matches_nomargin)
ROW_DATA_PATH <- ProjectPath()
DATA_PATH <- ProjectPath()
tickets_processed <- LoadObject()
View(tickets_processed)
tickets <- LoadObject()
View(tickets)
DATA_PATH <- ProjectPath()
DATA_PATH
View(tickets_processed)
matches_nomargin <- LoadObject()
View(matches_nomargin)
tickets_processed <- LoadObject()
View(tickets_processed)
library(dplyr)
library(dplyr)
tickets_processed %>% glimpse()
# Dependences ###
library(shinydashboard)
library(shiny)
library(glue)
library(dplyr)
library(purrr)
library(tictoc)
library(tidyr)
ProjectPath <- function(path = rstudioapi::getActiveProject()){
path <- rstudioapi::selectDirectory(caption="Select the raw file directory",path=path)
return(glue(path,"/"))
}
ProjectFile <- function(path = rstudioapi::getActiveProject()){
path <- rstudioapi::selectFile(caption="Select the raw file directory",path=path)
return(path)
}
LoadObject <- function(path = ProjectFile()) {
object <- read.csv(path)
return(object)
}
tickets_processed <- LoadObject("~/datbuyer/data/preprocessed/tickets_processed.csv")
View(tickets_processed)
analytics_processed <- LoadObject("~/datbuyer/data/preprocessed/analytics_processed.csv")
View(analytics_processed)
options(scipen = 999) # show full hashes
View(tickets_processed)
View(analytics_processed)
aggregs <- matches_nomargin %>% left_join(tickets_processed, by = c("Client_id" = "client"))
matches_nomargin <- LoadObject("~/datbuyer/output/matches/matches_frac_nomargin.csv")
View(matches_nomargin)
aggregs <- matches_nomargin %>% left_join(tickets_processed, by = c("Client_id" = "client"))
View(aggregs)
View(aggregs)
View(matches_nomargin)
View(aggregs)
runApp('Desktop/dashbuyer.R')
runApp('Desktop/dashbuyer.R')
runApp('Desktop/dashbuyer.R')
runApp('Desktop/dashbuyer.R')
runApp('Desktop/dashbuyer.R')
runApp('Desktop/dashbuyer.R')
runApp('Desktop/dashbuyer.R')
runApp('Desktop/dashbuyer.R')
format(120000)
format(120000, digits = 2)
format(120000, big.mark = ",")
runApp('Desktop/dashbuyer.R')
runApp('Desktop/dashbuyer.R')
runApp('Desktop/dashbuyer.R')
install.packages("DT")
install.packages("DT")
runApp('Desktop/dashbuyer.R')
runApp('Desktop/dashbuyer.R')
View(analytics_processed)
aggregs2 <- matches_nomargin %>% left_join(analytics_processed, by = c("Hash" = "hash"))
View(aggregs2)
gc()
# Set current working directory.
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# Set current working directory.
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
csv_name <- "based21.csv"
siniestros <- read.csv2(csv_name, sep = ';', stringsAsFactors = F)
siniestros <- siniestros$X0  # Convertimos de DataFrame a vector.
Histogram <- function(data) {
hist(data, breaks = length(unique(data)), col = "coral", border = "white", main = "Distribución del número de siniestros", xlab = "Siniestros")
}
Histogram(siniestros)
freqs <- as.data.frame(table(siniestros), stringsAsFactors = F)
freqs$prob <- freqs$Freq / sum(freqs$Freq)
freqs
mean(siniestros)
var(siniestros)
library(fitdistrplus)
library(gamlss)
fitP <- fitdist(siniestros, distr = "pois")
fitNB <- fitdist(siniestros, distr = "nbinom")
fitZIP <- fitdist(siniestros, distr = "ZIP", start = list(sigma = sum(siniestros==0)/length(siniestros), mu = mean(siniestros)))
stats <- gofstat(list(fitP, fitNB, fitZIP), fitnames = c("Poisson", "NegBinomial", "ZI-Poisson"))
stats
stats$aic[which.min(stats$aic)]  # Mejor distribución.
chi <- data.frame(pvalue = stats$chisqpvalue)
chi$result <- ifelse(chi$pvalue > 0.05, "The data do not follow the distribution (Reject H0)",
"You cannot conclude that the data do not follow the distribution (Fail to reject H0)")
chi
PlotFit <- function(data, dist) {
x <- as.numeric(data$siniestros)
y <- data$prob
leg.txt <- c("data", "fit")
plot(x, y, xlab = "data", ylab = "prob", main = "Empirical vs fitted distributions ", type = "l", col = "red", lwd = 1)
lines(x, dist, col = "blue", lwd = 1, lty=2)
legend("topright", leg = leg.txt, col = c("red","blue"), lty=1:2)
}
dens_P <- function(x) dpois(x, lambda = fitP$estimate)
dens_NB <- function(x) dnbinom(x, size = fitNB$estimate[[1]], mu = fitNB$estimate[[2]])
dens_ZIP <- function(x) dZIP(x, mu = fitZIP$estimate[[2]], sigma = fitZIP$estimate[[1]])
values <- as.numeric(freqs$siniestros)
dist_fit_P <- dens_P(values)
dist_fit_NB <- dens_NB(values)
dist_fit_ZIP <- dens_ZIP(values)
PlotFit(freqs, dist_fit_P)
PlotFit(freqs, dist_fit_NB)
PlotFit(freqs, dist_fit_ZIP)
# Probabilidad de cero siniestros
distribuciones <- c("Poisson", "NegBinomial", "ZI-Poisson", "DATA")
prob_0_siniestros <- c(dens_P(0), dens_NB(0), dens_ZIP(0), subset(freqs, freqs$siniestros == 0)$prob)
# Esperanza del número de siniestros
# En Poisson, E[x] = lambda
# En Negative Binomial, B(r,p), E[x] = p·r / (1 - p)
# En ZIP, E[x] = (1 - pi)*lambda
siniestros_esperados <- c(fitP$estimate[[1]], fitNB$estimate[[2]], (1 - fitZIP$estimate[[1]])*fitZIP$estimate[[2]], mean(siniestros))
(resultados <- data.frame(distribuciones, prob_0_siniestros, siniestros_esperados))
library(actuar)
dzero_modified_poisson <- function(x, lambda, p0) dzmpois(x, lambda, p0)
pzero_modified_poisson <- function(x, lambda, p0) dzmpois(x, lambda, p0)
fitZMP <- fitdist(siniestros, distr = "zero_modified_poisson", start = list(p0 = sum(siniestros==0)/length(siniestros), lambda = mean(siniestros)))
gofstat(fitZMP)
gofstat(fitZIP)  ## Compare it is same AIC but the other things are different!
dens_ZMP <- function(x) dzmpois(x, lambda = fitZMP$estimate[[2]], p0 = fitZMP$estimate[[1]])
dist_fit_ZMP <- dens_ZMP(values)
PlotFit(freqs, dist_fit_ZMP)
distribuciones <- c(distribuciones, "ZeroModifiedPoisson")
prob_0_siniestros <- c(prob_0_siniestros, dens_ZMP(0))
lambda <- fitZMP$estimate[[2]]
zero_truncated_mean <- lambda / (1 - exp(1)^(-lambda))  # lambda / (1 - e^(-lambda))^2  ---> MAL!
# En Wikipedia sale sin el QUADRADO! (^2) !!!
siniestros_esperados <- c(siniestros_esperados, (1 - fitZMP$estimate[[1]])*zero_truncated_mean)
(resultados <- data.frame(distribuciones, prob_0_siniestros, siniestros_esperados))
csv_name <- "autos2021.csv"
autos <- read.csv2(csv_name, sep = ',', stringsAsFactors = T, dec = ".")
str(autos)
head(autos)
autos$nsin
Histogram(autos$nsin)
unique(autos$nsin)
freqs <- as.data.frame(table(autos$nsin), stringsAsFactors = F)
freqs$prob <- freqs$Freq / sum(freqs$Freq)
freqs
freqs <- as.data.frame(table(autos$nsin), stringsAsFactors = F)
freqs
freqs$prob <- freqs$Freq / sum(freqs$Freq)
freqs
freqs <- as.data.frame(table(autos$nsin), stringsAsFactors = F)
freqs$prob <- freqs$Freq / sum(freqs$Freq) * 100
freqs
freqs <- as.data.frame(table(siniestros), stringsAsFactors = F)
freqs$prob <- freqs$Freq / sum(freqs$Freq)
freqs
freqs <- as.data.frame(table(autos$nsin), stringsAsFactors = F)
freqs$prob <- round(freqs$Freq / sum(freqs$Freq), 7)
freqs
freqs <- as.data.frame(table(autos$nsin), stringsAsFactors = F)
freqs$prob <- round(freqs$Freq / sum(freqs$Freq), 8)
freqs
csv_name <- "autos2021.csv"
autos <- read.csv2(csv_name, sep = ',', stringsAsFactors = T, dec = ".")
str(autos)
head(autos)
Histogram(autos$nsin)
freqs <- as.data.frame(table(autos$nsin), stringsAsFactors = F)
freqs$prob <- round(freqs$Freq / sum(freqs$Freq), 8)
freqs
